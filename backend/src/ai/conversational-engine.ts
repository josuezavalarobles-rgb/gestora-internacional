/**
 * ========================================
 * MOTOR CONVERSACIONAL DE IA
 * ========================================
 * Sistema avanzado de conversaci√≥n usando OpenAI y Langchain
 * con manejo de contexto, memoria y clasificaci√≥n inteligente
 */

import { ChatOpenAI } from 'langchain/chat_models/openai';
import { BufferMemory } from 'langchain/memory';
import { ConversationChain } from 'langchain/chains';
import { HumanMessage, SystemMessage, AIMessage, BaseMessage } from 'langchain/schema';
import { PromptTemplate } from 'langchain/prompts';
import OpenAI from 'openai';

import { logger } from '../utils/logger';
import { config } from '../config';
import { cacheService } from '../config/redis';

/**
 * Tipos e interfaces
 */
export interface ConversationContext {
  telefono: string;
  nombreUsuario?: string;
  etapa: 'inicial' | 'recopilando_info' | 'procesando' | 'en_seguimiento' | 'finalizada';
  datosRecopilados: {
    tipo?: 'garantia' | 'condominio';
    categoria?: string;
    descripcion?: string;
    urgencia?: boolean;
    unidad?: string;
    fotosRecibidas?: number;
  };
  historialMensajes: BaseMessage[];
  ultimoIntent?: string;
  casoActual?: string;
  metadata?: Record<string, any>;
}

export interface AIResponse {
  respuesta: string;
  intent: string;
  confidence: number;
  requiereHumano: boolean;
  razonEscalamiento?: string;
  datosRecopilados?: any;
  nuevaEtapa?: string;
  crearCaso?: boolean;
  accionSugerida?: string;
  emocionDetectada?: 'neutral' | 'frustrado' | 'urgente' | 'satisfecho' | 'confundido';
}

export interface ProcessMessageOptions {
  mensaje: string;
  contexto: ConversationContext;
  mediaUrl?: string;
  forceIntent?: string;
}

/**
 * Motor conversacional principal
 */
export class ConversationalEngine {
  private static instance: ConversationalEngine;
  private chatModel: ChatOpenAI;
  private openai: OpenAI;
  private conversations: Map<string, ConversationChain>;

  private constructor() {
    // Inicializar modelo de chat con Langchain
    this.chatModel = new ChatOpenAI({
      openAIApiKey: config.openai.apiKey,
      modelName: config.openai.model,
      temperature: config.openai.temperature,
      maxTokens: config.openai.maxTokens,
    });

    // Cliente OpenAI directo para funciones avanzadas
    this.openai = new OpenAI({
      apiKey: config.openai.apiKey,
    });

    // Mapa de conversaciones activas
    this.conversations = new Map();
  }

  /**
   * Singleton
   */
  public static getInstance(): ConversationalEngine {
    if (!ConversationalEngine.instance) {
      ConversationalEngine.instance = new ConversationalEngine();
    }
    return ConversationalEngine.instance;
  }

  /**
   * Procesar mensaje del usuario con contexto completo
   */
  public async processMessage(options: ProcessMessageOptions): Promise<AIResponse> {
    try {
      const { mensaje, contexto, mediaUrl } = options;

      logger.info(`ü§ñ Procesando mensaje de ${contexto.telefono}: "${mensaje}"`);

      // 1. Detectar urgencia primero
      const esUrgente = this.detectarUrgencia(mensaje);

      // 2. Detectar emoci√≥n del usuario
      const emocion = await this.detectarEmocion(mensaje);

      // 3. Construir mensajes para el modelo
      const messages = this.buildMessages(contexto, mensaje);

      // 4. Invocar modelo de IA
      const response = await this.chatModel.call(messages);

      // 5. Parsear respuesta
      const content = typeof response.content === 'string'
        ? response.content
        : JSON.stringify(response.content);
      const aiResponse = this.parseResponse(content, contexto);

      // 6. Agregar detecci√≥n de urgencia y emoci√≥n
      if (esUrgente && !aiResponse.requiereHumano) {
        aiResponse.requiereHumano = true;
        aiResponse.razonEscalamiento = 'Situaci√≥n urgente detectada';
        aiResponse.emocionDetectada = 'urgente';
      } else {
        aiResponse.emocionDetectada = emocion;
      }

      // 7. Guardar en cache (TTL 30 minutos)
      await this.guardarContextoEnCache(contexto.telefono, {
        ...contexto,
        ultimoIntent: aiResponse.intent,
      });

      logger.info(
        `‚úÖ Respuesta generada - Intent: ${aiResponse.intent}, Confianza: ${aiResponse.confidence}`
      );

      return aiResponse;
    } catch (error) {
      logger.error('‚ùå Error en ConversationalEngine.processMessage:', error);

      return {
        respuesta:
          'Disculpa, tuve un problema procesando tu mensaje. ¬øPuedes reformularlo o escribir de nuevo?',
        intent: 'error',
        confidence: 0,
        requiereHumano: false,
        emocionDetectada: 'neutral',
      };
    }
  }

  /**
   * Construir mensajes para el modelo
   */
  private buildMessages(contexto: ConversationContext, mensaje: string): BaseMessage[] {
    const messages: BaseMessage[] = [];

    // System prompt
    messages.push(new SystemMessage(this.getSystemPrompt(contexto)));

    // Historial de conversaci√≥n (√∫ltimos 10 mensajes)
    const historial = contexto.historialMensajes.slice(-10);
    messages.push(...historial);

    // Mensaje actual
    messages.push(new HumanMessage(mensaje));

    return messages;
  }

  /**
   * System prompt din√°mico seg√∫n etapa de conversaci√≥n
   */
  private getSystemPrompt(contexto: ConversationContext): string {
    const basePrompt = `
Eres un asistente virtual inteligente de Amico Management, empresa dominicana que administra condominios.

PERSONALIDAD:
- Profesional pero c√°lido y emp√°tico
- Hablas en espa√±ol dominicano natural (tuteo: "¬øC√≥mo est√°s?", "¬øEn qu√© te ayudo?")
- Eres eficiente y resolutivo
- Usas emojis con moderaci√≥n (m√°ximo 2 por mensaje)
- Te anticipas a las necesidades del usuario

TU MISI√ìN:
Ayudar a residentes a reportar problemas t√©cnicos de manera conversacional y eficiente.

INFORMACI√ìN A RECOPILAR:
1. Tipo de problema:
   - Garant√≠a: Defectos de construcci√≥n (filtraciones, grietas, estructurales)
   - Condominio: Mantenimiento general (√°reas comunes, limpieza, servicios)

2. Categor√≠a:
   - Filtraciones/Humedad
   - Problemas el√©ctricos
   - Plomer√≠a
   - Puertas/Ventanas
   - Pisos/Paredes/Techo
   - Aires acondicionados
   - √Åreas comunes
   - Seguridad
   - Otro

3. Descripci√≥n clara del problema
4. Urgencia/Severidad
5. Fotos/Videos (si es posible)
6. Unidad del residente

REGLAS DE ORO:
- NO uses lenguaje de formulario. S√© conversacional y natural
- Detecta informaci√≥n impl√≠cita (ej: "tengo filtraci√≥n" ‚Üí autom√°ticamente sabes que es categor√≠a Filtraciones)
- Confirma solo informaci√≥n importante
- Si el usuario se frustra o usa palabras como "urgente", "emergencia", "supervisor" ‚Üí ESCALA A HUMANO
- Mant√©n contexto de la conversaci√≥n completa

RESPONDE SIEMPRE EN FORMATO JSON:
{
  "respuesta": "tu respuesta conversacional aqu√≠",
  "intent": "saludo | reportar_problema | solicitar_estado | urgente | consulta | confirmar_cita | otro",
  "confidence": 0.0-1.0,
  "requiere_humano": false,
  "razon_escalamiento": "solo si requiere_humano es true",
  "datos_recopilados": {
    "tipo": "garantia o condominio (si detectaste)",
    "categoria": "la categor√≠a (si detectaste)",
    "descripcion": "resumen del problema",
    "urgencia": true/false,
    "unidad": "n√∫mero de unidad si lo mencion√≥"
  },
  "nueva_etapa": "inicial | recopilando_info | procesando | finalizada",
  "crear_caso": false,
  "accion_sugerida": "programar_visita | solicitar_fotos | escalar | ninguna"
}`;

    // Contexto espec√≠fico de la etapa actual
    let etapaContext = '';

    switch (contexto.etapa) {
      case 'inicial':
        etapaContext = `
ETAPA: INICIAL
- Primera interacci√≥n con el usuario
- Saluda amablemente si corresponde
- Pregunta c√≥mo puedes ayudar de forma natural
- Detecta si quiere reportar un problema o consultar estado`;
        break;

      case 'recopilando_info':
        etapaContext = `
ETAPA: RECOPILANDO INFORMACI√ìN
- Ya sabes que quiere reportar un problema
- Datos ya recopilados: ${JSON.stringify(contexto.datosRecopilados)}
- Identifica qu√© informaci√≥n falta y pregunta SOLO por eso
- NO repitas preguntas ya respondidas
- Si tienes tipo, categor√≠a y descripci√≥n completa ‚Üí marca crear_caso: true`;
        break;

      case 'procesando':
        etapaContext = `
ETAPA: PROCESANDO
- El caso est√° siendo creado
- Confirma al usuario que recibiste su reporte
- Explica pr√≥ximos pasos
- Marca nueva_etapa: "finalizada"`;
        break;

      case 'en_seguimiento':
        etapaContext = `
ETAPA: SEGUIMIENTO
- El usuario tiene un caso activo: ${contexto.casoActual || 'Desconocido'}
- Puede estar preguntando por el estado
- Provee informaci√≥n de seguimiento si la tienes
- S√© emp√°tico y proactivo`;
        break;

      case 'finalizada':
        etapaContext = `
ETAPA: FINALIZADA
- El reporte fue completado
- Ofrece ayuda adicional
- Pregunta si necesita algo m√°s`;
        break;
    }

    // Agregar informaci√≥n del usuario si est√° disponible
    const userContext = contexto.nombreUsuario
      ? `\n\nNOMBRE DEL USUARIO: ${contexto.nombreUsuario}\n`
      : '';

    return basePrompt + '\n\n' + etapaContext + userContext;
  }

  /**
   * Parsear respuesta del modelo
   */
  private parseResponse(content: string, contexto: ConversationContext): AIResponse {
    try {
      // Intentar extraer JSON de la respuesta
      const jsonMatch = content.match(/\{[\s\S]*\}/);

      if (jsonMatch) {
        const parsed = JSON.parse(jsonMatch[0]);

        return {
          respuesta: parsed.respuesta || content,
          intent: parsed.intent || 'desconocido',
          confidence: parsed.confidence || 0.7,
          requiereHumano: parsed.requiere_humano || false,
          razonEscalamiento: parsed.razon_escalamiento,
          datosRecopilados: parsed.datos_recopilados,
          nuevaEtapa: parsed.nueva_etapa,
          crearCaso: parsed.crear_caso || false,
          accionSugerida: parsed.accion_sugerida,
        };
      }

      // Si no hay JSON v√°lido, usar contenido como respuesta
      return {
        respuesta: content,
        intent: 'desconocido',
        confidence: 0.5,
        requiereHumano: false,
      };
    } catch (error) {
      logger.error('‚ùå Error parseando respuesta IA:', error);

      return {
        respuesta: content,
        intent: 'error',
        confidence: 0.3,
        requiereHumano: false,
      };
    }
  }

  /**
   * Detectar urgencia en el mensaje
   */
  private detectarUrgencia(mensaje: string): boolean {
    const palabrasUrgentes = [
      'urgente',
      'emergencia',
      'grave',
      'peligro',
      'inundaci√≥n',
      'inundacion',
      'incendio',
      'riesgo',
      'cr√≠tico',
      'critico',
      'ya',
      'ahora mismo',
      'r√°pido',
      'rapido',
    ];

    const mensajeLower = mensaje.toLowerCase();
    return palabrasUrgentes.some((palabra) => mensajeLower.includes(palabra));
  }

  /**
   * Detectar emoci√≥n del usuario usando GPT
   */
  private async detectarEmocion(
    mensaje: string
  ): Promise<'neutral' | 'frustrado' | 'urgente' | 'satisfecho' | 'confundido'> {
    try {
      const prompt = `Analiza la siguiente mensaje y determina la emoci√≥n del usuario. Responde SOLO con una palabra: neutral, frustrado, urgente, satisfecho, o confundido.

Mensaje: "${mensaje}"

Emoci√≥n:`;

      const response = await this.openai.chat.completions.create({
        model: 'gpt-3.5-turbo',
        messages: [{ role: 'user', content: prompt }],
        max_tokens: 10,
        temperature: 0.3,
      });

      const emocion = response.choices[0]?.message?.content?.trim().toLowerCase();

      if (
        emocion === 'neutral' ||
        emocion === 'frustrado' ||
        emocion === 'urgente' ||
        emocion === 'satisfecho' ||
        emocion === 'confundido'
      ) {
        return emocion;
      }

      return 'neutral';
    } catch (error) {
      logger.error('‚ùå Error detectando emoci√≥n:', error);
      return 'neutral';
    }
  }

  /**
   * Clasificar tipo de problema usando keywords
   */
  public clasificarTipoProblema(descripcion: string): 'garantia' | 'condominio' | 'desconocido' {
    const palabrasGarantia = [
      'filtraci√≥n',
      'filtracion',
      'grieta',
      'humedad',
      'defecto',
      'construcci√≥n',
      'construccion',
      'pared agrietada',
      'techo',
      'estructura',
      'rajadura',
      'fisura',
    ];

    const palabrasCondominio = [
      '√°rea com√∫n',
      'area comun',
      'piscina',
      'gimnasio',
      'limpieza',
      'port√≥n',
      'porton',
      'vigilancia',
      'basura',
      'ascensor',
      'elevador',
      'parqueo',
      'estacionamiento',
    ];

    const descLower = descripcion.toLowerCase();

    if (palabrasGarantia.some((p) => descLower.includes(p))) {
      return 'garantia';
    }

    if (palabrasCondominio.some((p) => descLower.includes(p))) {
      return 'condominio';
    }

    return 'desconocido';
  }

  /**
   * Generar resumen de conversaci√≥n
   */
  public async generarResumenConversacion(mensajes: string[]): Promise<string> {
    try {
      const conversacion = mensajes.join('\n');

      const prompt = `Resume la siguiente conversaci√≥n de WhatsApp en m√°ximo 3 oraciones. Enf√≥cate en el problema reportado:

${conversacion}

Resumen:`;

      const response = await this.openai.chat.completions.create({
        model: 'gpt-3.5-turbo',
        messages: [{ role: 'user', content: prompt }],
        max_tokens: 150,
        temperature: 0.5,
      });

      return response.choices[0]?.message?.content?.trim() || 'Sin resumen disponible';
    } catch (error) {
      logger.error('‚ùå Error generando resumen:', error);
      return 'Error al generar resumen';
    }
  }

  /**
   * Extraer informaci√≥n estructurada de texto libre
   */
  public async extraerInformacionEstructurada(texto: string): Promise<{
    tipo?: 'garantia' | 'condominio';
    categoria?: string;
    descripcion?: string;
    urgencia?: boolean;
    unidad?: string;
  }> {
    try {
      const prompt = `Extrae informaci√≥n estructurada del siguiente reporte de problema. Responde SOLO en formato JSON:

Texto: "${texto}"

Formato de respuesta:
{
  "tipo": "garantia o condominio (si es claro)",
  "categoria": "filtraciones, electrico, plomeria, etc",
  "descripcion": "descripci√≥n concisa del problema",
  "urgencia": true/false,
  "unidad": "n√∫mero de unidad si se menciona"
}

JSON:`;

      const response = await this.openai.chat.completions.create({
        model: 'gpt-3.5-turbo',
        messages: [{ role: 'user', content: prompt }],
        max_tokens: 200,
        temperature: 0.3,
      });

      const content = response.choices[0]?.message?.content?.trim() || '{}';
      const jsonMatch = content.match(/\{[\s\S]*\}/);

      if (jsonMatch) {
        return JSON.parse(jsonMatch[0]);
      }

      return {};
    } catch (error) {
      logger.error('‚ùå Error extrayendo informaci√≥n:', error);
      return {};
    }
  }

  /**
   * Guardar contexto en Redis cache
   */
  private async guardarContextoEnCache(
    telefono: string,
    contexto: ConversationContext
  ): Promise<void> {
    try {
      await cacheService.set(`conversation:${telefono}`, contexto, 1800); // 30 minutos
    } catch (error) {
      logger.error('‚ùå Error guardando contexto en cache:', error);
    }
  }

  /**
   * Obtener contexto desde cache
   */
  public async obtenerContextoDeCache(telefono: string): Promise<ConversationContext | null> {
    try {
      return await cacheService.get<ConversationContext>(`conversation:${telefono}`);
    } catch (error) {
      logger.error('‚ùå Error obteniendo contexto de cache:', error);
      return null;
    }
  }

  /**
   * Limpiar conversaci√≥n del cache
   */
  public async limpiarConversacion(telefono: string): Promise<void> {
    try {
      await cacheService.del(`conversation:${telefono}`);
      this.conversations.delete(telefono);
    } catch (error) {
      logger.error('‚ùå Error limpiando conversaci√≥n:', error);
    }
  }
}

// Exportar instancia √∫nica
export const conversationalEngine = ConversationalEngine.getInstance();

export default ConversationalEngine;
